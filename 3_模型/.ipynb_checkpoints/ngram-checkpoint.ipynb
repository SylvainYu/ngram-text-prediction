{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "n = 3  # 3-gram\n",
    "\n",
    "data_path       = '../2_预处理/data/'          # 存放预处理后新闻数据的目录\n",
    "wordtable_path  = '../2_预处理/wordtable.txt'  # 词表路径\n",
    "stopwords_path  = '../2_预处理/stopwords/中文停用词表.txt' # 停止词表路径\n",
    "testset_path    = './testset/'       # 测试集的目录\n",
    "prediction_path = './predictions/'   # 存放预测结果的目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list = []  # n元组（分子）\n",
    "prefix_list = []  # n-1元组（分母）\n",
    "\n",
    "# 遍历所有预处理过的新闻文件\n",
    "for i, datafile in enumerate(os.listdir(data_path)):\n",
    "    with open(data_path + datafile, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentence = ['<BOS>'] + line.split() + ['<EOS>']  # 列表，形如：['<BOS>', '显得', '十分', '明亮', '<EOS>']\n",
    "            ngrams = list(zip(*[sentence[i:] for i in range(n)]))   # 一个句子中n-gram元组的列表\n",
    "            prefix = list(zip(*[sentence[i:] for i in range(n-1)])) # 历史前缀元组的列表\n",
    "            ngrams_list += ngrams\n",
    "            prefix_list += prefix\n",
    "\n",
    "ngrams_counter = Counter(ngrams_list)\n",
    "prefix_counter = Counter(prefix_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []  # 词表中的全部词\n",
    "with open(wordtable_path, encoding='utf-8') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        all_words.append(line.split()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停止词\n",
    "with open(stopwords_path, encoding='utf-8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = set(map(lambda x:x.strip(), stopwords))  # 去除末尾换行符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(sentence):\n",
    "    \"\"\"\n",
    "    计算一个句子的概率。\n",
    "    Params:\n",
    "        sentence: 由词构成的列表表示的句子。\n",
    "    Returns:\n",
    "        句子的概率。\n",
    "    \"\"\"\n",
    "    prob = 1  # 初始化句子概率\n",
    "    ngrams = list(zip(*[sentence[i:] for i in range(n)]))   # 将句子处理成n-gram的列表\n",
    "    for ngram in ngrams:\n",
    "        # 累乘每个n-gram的概率，并使用加一法进行数据平滑\n",
    "        prob *= (1 + ngrams_counter[ngram]) / (len(prefix_counter) + prefix_counter[(ngram[0], ngram[1])])\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pre_sentence, post_sentence, all_words, cand_num=1):\n",
    "    \"\"\"\n",
    "    根据历史进行一个词的预测。\n",
    "    Params:\n",
    "        pre_sentence: 待预测词之前部分句子的分词结果构成的列表。\n",
    "        post_sentence: 待预测词之后部分句子的分词结果构成的列表。\n",
    "        all_words: 所有候选词构成的列表。\n",
    "        cand_num: 候选词数，默认为1。\n",
    "    Returns:\n",
    "        一个含有cand_num个元素的列表，表示预测的词，概率由大到小排序；\n",
    "        如果预测失败，返回None。\n",
    "    \"\"\"\n",
    "    word_prob = []  # 候选词及其概率构成的元组的列表\n",
    "    for word in all_words:\n",
    "        # 实际上不需要算整个句子的概率，只需要算待预测词附近的概率即可，因为句子其他部分的概率不受待预测词影响\n",
    "        test_sentence = pre_sentence[-(n-1):] + [word] + post_sentence[:(n-1)]  # 待预测词及其前后各n-1个词的列表\n",
    "        word_prob.append( (word, probability(test_sentence)) )                  # (词, 概率)元组构成的列表\n",
    "\n",
    "    return sorted(word_prob, key=lambda tup: tup[1], reverse=True)[:cand_num]  # 按概率降序排序并取前cand_num个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.633 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 总而言之，从大环境上来说，目前仍然是处于教育用户的阶段，加上扫地 [机器人] 品类的下限很低，市场还没有形成一个足够清晰的认知。\n",
      "18 在三大运营商积极展开5G部署工作的同时，各大手机 [厂商] 也在积极布局5G产品，说到这里就不能不提到OPPO了。\n",
      "60 消息人士此前告诉外媒称，诺依曼有权以高达9.7亿美元的 [价格] 出售其在该公司的股份，作为软银将从投资者和员工手中购买高达30亿美元的WeWork股份的要约的一部分。\n",
      "73 苏宁金服相关负责人在接受 [媒体] 采访时表示，《办法》出台后，公司也将积极配合监管机构开展相关工作。\n"
     ]
    }
   ],
   "source": [
    "# 加载测试集标签（答案）\n",
    "with open('testset/answer.txt', encoding='utf-8') as f:\n",
    "    answers = [answer.strip() for answer in f]  # 答案构成的列表\n",
    "    \n",
    "prediction_file = open(prediction_path + 'prediction_ngram.txt', 'w', encoding='utf-8')  # 存放预测结果\n",
    "\n",
    "# 开始测试\n",
    "correct_count = 0  # 预测正确的数量\n",
    "\n",
    "with open('testset/questions.txt', encoding='utf-8') as f:\n",
    "    questions = f.readlines()  # 测试集规模\n",
    "    total_count = len(questions)\n",
    "    for i, question in enumerate(questions):\n",
    "        question = question.strip()\n",
    "        pre_mask = question[:question.index('[MASK]')]     # 待预测词的历史\n",
    "        post_mask = question[question.index('[MASK]')+6:]  # 待预测词后的剩余部分\n",
    "        \n",
    "        pre_sentence = jieba.cut(pre_mask.replace('，', ' '))  # 分词\n",
    "        post_sentence = jieba.cut(post_mask.replace('，', ' '))  # 分词\n",
    "        pre_sentence = [word.strip() for word in pre_sentence if word.strip() and word not in stopwords]  # 去除停止词、空串\n",
    "        post_sentence = [word.strip() for word in post_sentence if word.strip() and word not in stopwords]  # 去除停止词、空串\n",
    "\n",
    "        predict_cand = predict(pre_sentence, post_sentence, all_words)  # 预测一个概率最大的词\n",
    "        prediction_file.write(' '.join([w[0] for w in predict_cand]) + '\\n')  # 将预测结果写入文件\n",
    "\n",
    "        # 遍历多个预测结果\n",
    "        for j, p in enumerate(predict_cand):\n",
    "            if p[0] == answers[i]:\n",
    "                print(i, '{} [{}] {}'.format(pre_mask, p[0], post_mask))\n",
    "                correct_count += 1\n",
    "                break\n",
    "                    \n",
    "prediction_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率：4/100\n"
     ]
    }
   ],
   "source": [
    "print('准确率：{}/{}'.format(correct_count, total_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
